{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake In A Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __[History / Temporality / Time Travel](#History-/-Temporality-/-Time-Travel)__\n",
    "- __[Schema Validation](#Schema-Validation)__\n",
    "- __[Vacuum](#Vacuum)__\n",
    "- __[Delta Lake API](#Delta-Lake-API-(Update,-Merge,-Delete))__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('delta_fun').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [ ['Maciej', 29], ['John', 31], ['Jack',35]]\n",
    "data2 = [ ['Beck', 24], ['Wick', 21], ['Samantha',35]]\n",
    "data3 = [ ['Homer', 23, 9], ['Barney', 24, 7], ['Edward', 45, 8]]\n",
    "dataSchema = StructType([ \\\n",
    "    StructField(\"name\", StringType()), \\\n",
    "    StructField(\"age\", IntegerType()) \\\n",
    "    ])\n",
    "differentSchema = StructType([ \\\n",
    "    StructField(\"name\", StringType()), \\\n",
    "    StructField(\"age\", IntegerType()), \\\n",
    "    StructField(\"foot_size\", IntegerType()) \\\n",
    "    ])\n",
    "df1 = spark.sparkContext.parallelize(data1).toDF(dataSchema)\n",
    "df2 = spark.sparkContext.parallelize(data2).toDF(dataSchema)\n",
    "df3 = spark.sparkContext.parallelize(data3).toDF(differentSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History / Temporality / Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/example_1/old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Maciej| 29|\n",
      "|  John| 31|\n",
      "|  Jack| 35|\n",
      "+------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|    Beck| 24|\n",
      "|    Wick| 21|\n",
      "|Samantha| 35|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Maciej| 29|\n",
      "|  John| 31|\n",
      "|  Jack| 35|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"overwrite\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Samantha| 35|\n",
      "|    Wick| 21|\n",
      "|    Beck| 24|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems:\n",
    "- We lost previous values\n",
    "- We can't go back\n",
    "- In case of mistake we are in trouble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/example_1/delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Maciej| 29|\n",
      "|  Jack| 35|\n",
      "|  John| 31|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark streaming part 1 - come back here later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_path = './data/example_1/delta_stream'\n",
    "streaming_checkpoint = './data/example_1/delta_stream_checkpount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = spark.readStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"ignoreChanges\", \"true\") \\\n",
    "  .load(path) \\\n",
    "  .agg(avg(col('age')).alias(\"average\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .option(\"checkpointLocation\", streaming_checkpoint) \\\n",
    "  .start(streaming_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, you can continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/example_1/delta | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"overwrite\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Samantha| 35|\n",
      "|    Beck| 24|\n",
      "|    Wick| 21|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/example_1/delta | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.mode(\"append\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Samantha| 35|\n",
      "|  Maciej| 29|\n",
      "|    Jack| 35|\n",
      "|    Beck| 24|\n",
      "|    John| 31|\n",
      "|    Wick| 21|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/example_1/delta | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|Samantha| 35|\n",
      "|  Maciej| 29|\n",
      "|  Maciej| 29|\n",
      "|    Jack| 35|\n",
      "|    Jack| 35|\n",
      "|    John| 31|\n",
      "|    John| 31|\n",
      "|    Wick| 21|\n",
      "|    Beck| 24|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet format also works, but show all values (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-06 20:56:18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Append', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-06 20:56:14</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-06 20:56:10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'ErrorIfExists', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version           timestamp userId userName operation  \\\n",
       "0        2 2020-02-06 20:56:18   None     None     WRITE   \n",
       "1        1 2020-02-06 20:56:14   None     None     WRITE   \n",
       "2        0 2020-02-06 20:56:10   None     None     WRITE   \n",
       "\n",
       "                              operationParameters   job notebook clusterId  \\\n",
       "0         {'mode': 'Append', 'partitionBy': '[]'}  None     None      None   \n",
       "1      {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "2  {'mode': 'ErrorIfExists', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \n",
       "0          1.0           None           True  \n",
       "1          0.0           None          False  \n",
       "2          NaN           None           True  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Maciej| 29|\n",
      "|  Jack| 35|\n",
      "|  John| 31|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to add DataFrame with different schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+\n",
      "|  name|age|foot_size|\n",
      "+------+---+---------+\n",
      "| Homer| 23|        9|\n",
      "|Barney| 24|        7|\n",
      "|Edward| 45|        8|\n",
      "+------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'A schema mismatch detected when writing to the Delta table.\\nTo enable schema migration, please set:\\n\\'.option(\"mergeSchema\", \"true\")\\'.\\n\\nTable schema:\\nroot\\n-- name: string (nullable = true)\\n-- age: integer (nullable = true)\\n\\n\\nData schema:\\nroot\\n-- name: string (nullable = true)\\n-- age: integer (nullable = true)\\n-- foot_size: integer (nullable = true)\\n\\n         \\nIf Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE\\ncommand for changing the schema.\\n        ;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1197.save.\n: org.apache.spark.sql.AnalysisException: A schema mismatch detected when writing to the Delta table.\nTo enable schema migration, please set:\n'.option(\"mergeSchema\", \"true\")'.\n\nTable schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n\n\nData schema:\nroot\n-- name: string (nullable = true)\n-- age: integer (nullable = true)\n-- foot_size: integer (nullable = true)\n\n         \nIf Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE\ncommand for changing the schema.\n        ;\n\tat org.apache.spark.sql.delta.MetadataMismatchErrorBuilder.finalizeAndThrow(DeltaErrors.scala:795)\n\tat org.apache.spark.sql.delta.schema.ImplicitMetadataOperation$class.updateMetadata(ImplicitMetadataOperation.scala:111)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:45)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:84)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:65)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(WriteIntoDelta.scala:64)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:388)\n\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:64)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:131)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.GeneratedMethodAccessor74.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-f07d31a5f6e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'A schema mismatch detected when writing to the Delta table.\\nTo enable schema migration, please set:\\n\\'.option(\"mergeSchema\", \"true\")\\'.\\n\\nTable schema:\\nroot\\n-- name: string (nullable = true)\\n-- age: integer (nullable = true)\\n\\n\\nData schema:\\nroot\\n-- name: string (nullable = true)\\n-- age: integer (nullable = true)\\n-- foot_size: integer (nullable = true)\\n\\n         \\nIf Table ACLs are enabled, these options will be ignored. Please use the ALTER TABLE\\ncommand for changing the schema.\\n        ;'"
     ]
    }
   ],
   "source": [
    "df3.write.mode(\"append\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can't, unless we set option \"mergeSchema\" to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.mode(\"append\").option(\"mergeSchema\", \"true\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+------------------------------------------+\n",
      "|version|timestamp          |operation|operationParameters                       |\n",
      "+-------+-------------------+---------+------------------------------------------+\n",
      "|3      |2020-02-06 20:57:46|WRITE    |[mode -> Append, partitionBy -> []]       |\n",
      "|2      |2020-02-06 20:56:18|WRITE    |[mode -> Append, partitionBy -> []]       |\n",
      "|1      |2020-02-06 20:56:14|WRITE    |[mode -> Overwrite, partitionBy -> []]    |\n",
      "|0      |2020-02-06 20:56:10|WRITE    |[mode -> ErrorIfExists, partitionBy -> []]|\n",
      "+-------+-------------------+---------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().select(\"version\",\"timestamp\",\"operation\",\"operationParameters\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- foot_size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like deltaTable store old metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|  Edward| 45|\n",
      "|  Barney| 24|\n",
      "|   Homer| 23|\n",
      "|Samantha| 35|\n",
      "|  Maciej| 29|\n",
      "|    Wick| 21|\n",
      "|    Beck| 24|\n",
      "|    Jack| 35|\n",
      "|    John| 31|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    name|age|foot_size|\n",
      "+--------+---+---------+\n",
      "|  Edward| 45|        8|\n",
      "|  Barney| 24|        7|\n",
      "|   Homer| 23|        9|\n",
      "|Samantha| 35|     null|\n",
      "|  Maciej| 29|     null|\n",
      "|    Wick| 21|     null|\n",
      "|    Beck| 24|     null|\n",
      "|    Jack| 35|     null|\n",
      "|    John| 31|     null|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, path)\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History versions are not affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(path).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vacuum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clear history we don't need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/example_1/delta | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/example_1/delta | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+------------------------------------------+\n",
      "|version|timestamp          |operation|operationParameters                       |\n",
      "+-------+-------------------+---------+------------------------------------------+\n",
      "|3      |2020-02-06 20:57:46|WRITE    |[mode -> Append, partitionBy -> []]       |\n",
      "|2      |2020-02-06 20:56:18|WRITE    |[mode -> Append, partitionBy -> []]       |\n",
      "|1      |2020-02-06 20:56:14|WRITE    |[mode -> Overwrite, partitionBy -> []]    |\n",
      "|0      |2020-02-06 20:56:10|WRITE    |[mode -> ErrorIfExists, partitionBy -> []]|\n",
      "+-------+-------------------+---------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().select(\"version\",\"timestamp\",\"operation\",\"operationParameters\").show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing have changed because default retension is 7 days ^-^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake API (Update, Merge, Delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    name|age|foot_size|\n",
      "+--------+---+---------+\n",
      "|  Edward| 45|        8|\n",
      "|  Barney| 24|        7|\n",
      "|   Homer| 23|        9|\n",
      "|Samantha| 35|     null|\n",
      "|  Maciej| 29|     null|\n",
      "|    Wick| 21|     null|\n",
      "|    Beck| 24|     null|\n",
      "|    Jack| 35|     null|\n",
      "|    John| 31|     null|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete(\"name = 'Edward'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    name|age|foot_size|\n",
      "+--------+---+---------+\n",
      "|  Barney| 24|        7|\n",
      "|   Homer| 23|        9|\n",
      "|Samantha| 35|     null|\n",
      "|  Maciej| 29|     null|\n",
      "|    Wick| 21|     null|\n",
      "|    Beck| 24|     null|\n",
      "|    Jack| 35|     null|\n",
      "|    John| 31|     null|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\n",
    "    condition = \"foot_size is null\",\n",
    "    set = { \"foot_size\": \"6\" } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    name|age|foot_size|\n",
      "+--------+---+---------+\n",
      "|Samantha| 35|        6|\n",
      "|  Maciej| 29|        6|\n",
      "|  Barney| 24|        7|\n",
      "|    John| 31|        6|\n",
      "|    Jack| 35|        6|\n",
      "|    Beck| 24|        6|\n",
      "|    Wick| 21|        6|\n",
      "|   Homer| 23|        9|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this example a little bit less syntetic and assume that 'name' is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.toDF().withColumn(\"count\",lit(1))\\\n",
    "    .write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+-----+\n",
      "|    name|age|foot_size|count|\n",
      "+--------+---+---------+-----+\n",
      "|Samantha| 35|        6|    1|\n",
      "|  Maciej| 29|        6|    1|\n",
      "|  Barney| 24|        7|    1|\n",
      "|    John| 31|        6|    1|\n",
      "|    Jack| 35|        6|    1|\n",
      "|    Beck| 24|        6|    1|\n",
      "|    Wick| 21|        6|    1|\n",
      "|   Homer| 23|        9|    1|\n",
      "+--------+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"clients\").merge(\n",
    "    source = df3.alias(\"new_clients\"),\n",
    "    condition = expr(\"clients.name == new_clients.name\")\n",
    "  ).whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"count\": col(\"clients.count\") + 1\n",
    "    }\n",
    "  ).whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"name\": col(\"new_clients.name\"),\n",
    "      \"age\": col(\"new_clients.age\"),\n",
    "      \"foot_size\": col(\"new_clients.foot_size\"),\n",
    "      \"count\": lit(\"1\")\n",
    "    }\n",
    "  ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+-----+\n",
      "|    name|age|foot_size|count|\n",
      "+--------+---+---------+-----+\n",
      "|Samantha| 35|        6|    1|\n",
      "|  Maciej| 29|        6|    1|\n",
      "|  Edward| 45|        8|    1|\n",
      "|  Barney| 24|        7|    2|\n",
      "|   Homer| 23|        9|    2|\n",
      "|    Jack| 35|        6|    1|\n",
      "|    Beck| 24|        6|    1|\n",
      "|    Wick| 21|        6|    1|\n",
      "|    John| 31|        6|    1|\n",
      "+--------+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta as Streaming Sink - part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_path = './data/example_1/delta_stream'\n",
    "streaming_checkpoint = './data/example_1/delta_stream_checkpount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-267-d7a36b652170>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-267-d7a36b652170>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    agg(avg(col(\"age\"))) \\\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stream = spark.readStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .load(path) \\\n",
    "  .agg(avg(col(\"age\"))) \\\n",
    "  .writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .option(\"checkpointLocation\", streaming_checkpoint) \\\n",
    "  .start(streaming_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQueryManager at 0x7faa713938d0>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
